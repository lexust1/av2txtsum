{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f505e7b-743a-452a-914c-eb470388022e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "To-do:\n",
    "- Audio extraction from video file\n",
    "- Find subtitles.\n",
    "- Audio to text.\n",
    "- Open model and change the last layers.\n",
    "- Own simple models.\n",
    "- Summary for topiic.\n",
    "- Detailed summary of big text.\n",
    "- Work with big text to summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635c848d-39ac-4908-b000-c1db9e7d994a",
   "metadata": {},
   "source": [
    "# Data source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377d7146-a3e6-494e-a514-493c0e1cfcec",
   "metadata": {},
   "source": [
    "We will use a [video](https://www.youtube.com/watch?v=FrDnPTPgEmk) on YouTube. This video, created by IBM Research (Generative AI for Business, IBM Think 2023, Dario Gil), is about generative AI and is published under a Creative Commons Licence ([CC BY](https://support.google.com/youtube/answer/2797468)).\n",
    "\n",
    "- Download the [video](https://www.youtube.com/watch?v=FrDnPTPgEmk) and save it in the `data` folder.\n",
    "- Copy the transcript and save it as `transcript.txt` in the same folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e16d677-7a81-4c44-a635-261cd7992ae0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/FrDnPTPgEmk\" \n",
       "    frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; \n",
       "    encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "# The YouTube video ID\n",
    "video_id = \"FrDnPTPgEmk\"\n",
    "# Creating the HTML iframe code\n",
    "iframe_code = f\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/{video_id}\" \n",
    "    frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; \n",
    "    encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "</div>\n",
    "\"\"\"\n",
    "# Displaying the YouTube video\n",
    "HTML(iframe_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067d031e-a648-49c7-9d41-5018084dc19a",
   "metadata": {},
   "source": [
    "## Convert the txt transcript into an SRT (SubRip Subtitle) file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adebd5e-d4b4-4f1d-9c32-5001384ccda9",
   "metadata": {},
   "source": [
    "Convert the TXT transcript into an [SRT (SubRip Subtitle)](https://docs.fileformat.com/video/srt/) file to enable the use of subtitles in various media players, including VLC (see the [txt_to_srt module](utils/txt_to_srt.py))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d8294ba-7810-463f-84a9-212e2afbcdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.txt_to_srt import txt_to_srt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6100f096-0bc2-445b-a3bb-01f7e451c782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "00:00:09,000 --> 00:00:17,000\n",
      ">> Welcome to IBM THINK 2023!\n",
      "\n",
      "2\n",
      "00:00:17,000 --> 00:00:23,000\n",
      ">> AI generated art, AI generated songs.\n",
      "\n",
      "3\n",
      "00:00:23,000 --> 00:00:31,000\n",
      "AI, what is that? It sure is a lot of fun. But when foundation models are applied to big business, well,\n",
      "\n",
      "4\n",
      "00:00:31,000 --> 00:00:36,000\n",
      "you need to think bigger. Because AI and business needs to be held to a higher standard.\n",
      "\n",
      "5\n",
      "00:00:36,000 --> 00:00:42,000\n",
      "Built to be trusted, secured, and adaptable. This isn't simple automation that is only\n",
      "\n",
      "6\n",
      "00:00:42,000 --> 00:00:48,000\n",
      "trained to do one thing. This is AI that is built and focused to work across your organization.\n",
      "\n",
      "7\n",
      "00:00:48,000 --> 00:00:54,000\n",
      "This isn't committing to a single system. This is hybrid ready AI that can scale across your systems.\n",
      "\n",
      "8\n",
      "00:00:54,000 --> 00:01:00,000\n",
      "This isn't wondering where an answer came from. This is AI that can show its work.\n",
      "\n",
      "9\n",
      "00:01:00,000 --> 00:01:07,000\n",
      "When you build AI into the core of your business, you can go so much fu\n"
     ]
    }
   ],
   "source": [
    "srt = txt_to_srt(\"data/transcript.txt\", \"data/transcript.srt\")\n",
    "print(srt[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd81ddac-fb4f-4405-aaaa-08519b885d2d",
   "metadata": {},
   "source": [
    "## Download and Install FFmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0a7bd0-b29b-4d74-bfb4-2b185def7d9c",
   "metadata": {},
   "source": [
    "[Download](https://www.ffmpeg.org/download.html) the FFmpeg multimedia framework for selected OS (MacOS in our case)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8122b1a5-4442-494f-a33c-c3cd38da9d10",
   "metadata": {},
   "source": [
    "Use Terminal and Homebrew:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b4e886-55f7-43ef-b1a1-656d3ec7c509",
   "metadata": {},
   "source": [
    "```bash\n",
    "brew update\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dff6ce-3831-4c1c-9622-a8482074c4f6",
   "metadata": {},
   "source": [
    "```bash\n",
    "brew upgrade\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb9e7d1-3530-4616-9a6d-3a7f7dba3f76",
   "metadata": {},
   "source": [
    "```bash\n",
    "brew install ffmpeg\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a03746-03d2-45c1-9088-9ea62863e1be",
   "metadata": {},
   "source": [
    "## Install pydub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd370da7-3fb3-49e6-aabb-35ace84d75c7",
   "metadata": {},
   "source": [
    "Install pydub.\n",
    "\n",
    "`pydub` is a Python library that can be used for audio processing tasks (uses ffmpeg under hood). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bc4e6f1-92a8-4cf7-8fb7-fae89925fb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydub in /Users/lex/opt/miniconda3/envs/env_mc_tf/lib/python3.9/site-packages (0.25.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pydub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2014f37a-f4de-463d-af10-ba2652217c84",
   "metadata": {},
   "source": [
    "## Extract audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b11a40-0384-4e9a-a376-60477800a173",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Many ASR libraries and models use the following [requirements](https://github.com/ggerganov/whisper.cpp#quick-start) for audio:\n",
    "- the format file: wav\n",
    "- the bit depth: 16 bit\n",
    "- the frame rate: 16 kHz\n",
    "- channels: 1 (mono)\n",
    "\n",
    "Extract audio from video using [the extract_audio.py](utils/extract_audio.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37a231b4-c65c-4f41-be88-65f36b93f2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.extract_audio import extract_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a60a212-0ccb-4355-b100-69ceb71bce7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_name': 'data/audio.wav',\n",
       " 'duration': '2060.539 s',\n",
       " 'channels': 1,\n",
       " 'frame_rate': '16000 Hz',\n",
       " 'bit_depth': '16 bit'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_audio(\"data/Generative AI for business.mp4\", \"data/audio.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684c7ab2-0f1a-4120-8e24-493300c15afc",
   "metadata": {},
   "source": [
    "## Audio to text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc390516-92b5-4371-aded-c0a0f2a66c36",
   "metadata": {},
   "source": [
    "There are several strategies for converting an audio file to a text file:\n",
    "\n",
    "1. **Cloud Services Offered by Big Tech Companies**:\n",
    "   - [Google Speech-to-Text](https://cloud.google.com/speech-to-text/)\n",
    "   - [IBM Watson Speech to Text](https://www.ibm.com/products/speech-to-text)\n",
    "   - [Microsoft Azure Speech to Text](https://azure.microsoft.com/en-us/products/ai-services/speech-to-text)\n",
    "   - [Amazon Transcribe](https://aws.amazon.com/transcribe/)\n",
    "   - [Yandex SpeechKit](https://cloud.yandex.com/en/services/speechkit)\n",
    "   \n",
    "   These companies offer various plans with user-friendly interfaces. You can upload an audio file and receive the transcription without any additional steps. However, these services can be quite expensive for large volumes of audio. Some offer a limited amount of free minutes each month, but this is often insufficient for substantial projects. Additionally, some companies prefer not to use cloud services due to security concerns.\n",
    "\n",
    "2. **Built-in Services in Operating Systems on Smartphones and Laptops** \n",
    "\n",
    "   MacOS, iOS, Windows, Android, etc., have built-in options for working with audio. However, they are generally more focused on speech-to-text than audio-to-text. These tools are tied to specific operating systems, which can be inconvenient in some instances.\n",
    "\n",
    "4. **Professional Transcription Software** \n",
    "\n",
    "   Examples include Express Scribe, InqScribe, Riverside, etc. Most of these are paid and proprietary.\n",
    "\n",
    "6. **Various Apps and Online Services for Different Platforms** \n",
    "\n",
    "   Many of them have low quality or collect user data.\n",
    "\n",
    "8. **Open Source Projects, Libraries, and Models (Most Work Offline)**:\n",
    "   - [Mozilla DeepSpeech](https://github.com/mozilla/DeepSpeech)\n",
    "   - [OpenAI Whisper](https://github.com/openai/whisper)\n",
    "   - [SeamlessM4T](https://github.com/facebookresearch/seamless_communication)\n",
    "   - [Kaldi](https://github.com/kaldi-asr/kaldi)\n",
    "   - [Vosk](https://github.com/alphacep/vosk-api)\n",
    "   - [PocketSphinx](https://github.com/cmusphinx/pocketsphinx)\n",
    "   - [Julius](https://github.com/julius-speech/julius)\n",
    "  \n",
    "    These tools are often developed by communities of researchers and engineers. They provide a cost-effective way for anyone to transcribe audio, especially since they work offline, ensuring data privacy. However, they might require technical expertise to set up and use.\n",
    "\n",
    "9. **Fine-Tuning an Open Model**\n",
    "\n",
    "   This involves adjusting the parameters of an existing speech-to-text model to improve its performance, often using a new dataset that's more relevant to the specific use case. It requires some machine learning expertise and resources but can lead to more accurate transcriptions.\n",
    "11. **Building Your Own Model**\n",
    "\n",
    "    For those with the necessary resources and expertise, creating a custom speech-to-text model from scratch allows for the most control over the transcription process, accommodating unique requirements and data privacy concerns. However, this is the most resource-intensive option and requires a deep understanding of machine learning and audio processing.\n",
    "\n",
    "We will focus on points 1, 5, and 6.\n",
    "\n",
    "Useful libraries and models for our goal:\n",
    "- [SpeechRecognition library](https://github.com/Uberi/speech_recognition)\n",
    "- [Hugging Face](https://github.com/huggingface)\n",
    "- https://huggingface.co/facebook/wav2vec2-base-960h\n",
    "- https://jitsi.github.io/jiwer/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (env_mc_asr_312)",
   "language": "python",
   "name": "env_mc_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
